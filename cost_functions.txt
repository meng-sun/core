#cost functions 

#also try different weights
#try with different labels
#try training with different orders of functions

def mce(logits, labels):
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_true_logits = tf.SparseTensor(indices=indices, values=logits,shape=[batch_size, num_classes])
    true_logits = tf.sparse_tensor_to_dense(sparse_true_logits)    
    opp_indices = tf.cast(tf.pack((tf.range(0, batch_size), -labels+1), axis=1), dtype=tf.int64)
    sparse_false_logits = tf.SparseTensor(indices=opp_indices, values=logits,shape=[batch_size, num_classes])
    false_logits = tf.sparse_tensor_to_dense(sparse_false_logits)    
    return tf.pow(1+tf.exp(false_logits-true_logits),-1)


def lvq2(logits,labels, min_cost):
    cost = multiclass_hinge_loss(logits, labels)
    return tf.minimum(tf.constant(min_cost), cost)


def square_square(logits,labels):
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_true_logits = tf.SparseTensor(indices=indices, values=logits,shape=[batch_size, num_classes])
    true_logits = tf.sparse_tensor_to_dense(sparse_true_logits)    
    opp_indices = tf.cast(tf.pack((tf.range(0, batch_size), -labels+1), axis=1), dtype=tf.int64)
    sparse_false_logits = tf.SparseTensor(indices=opp_indices, values=logits,shape=[batch_size, num_classes])
    false_logits = tf.sparse_tensor_to_dense(sparse_false_logits)    
    return tf.pow(true_logits,2)-tf.pow(tf.maximum(tf.zeros(batch_num), 1-false_logits),2)


def square_exp(logits,labels,alpha=0.5):
	batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_true_logits = tf.SparseTensor(indices=indices, values=logits,shape=[batch_size, num_classes])
    true_logits = tf.sparse_tensor_to_dense(sparse_true_logits)    
    opp_indices = tf.cast(tf.pack((tf.range(0, batch_size), -labels+1), axis=1), dtype=tf.int64)
    sparse_false_logits = tf.SparseTensor(indices=opp_indices, values=logits,shape=[batch_size, num_classes])
    false_logits = tf.sparse_tensor_to_dense(sparse_false_logits)    
    return tf.pow(true_logits,2)+alpha*(tf.exp(-false_logits))


def nll(logits, labels):
    return 0    


def mee(logits,labels):
    return 0


def modified_cosine_difference(logits, labels, alpha=0.1):
    #otherwise with regular equation cosine_difference is too similar to MCE
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_targets = tf.SparseTensor(indices=indices, values=tf.ones(batch_size, dtype=tf.float32),shape=[batch_size, num_classes])
    targets = tf.sparse_tensor_to_dense(sparse_targets)

    '''turning targets from 0,1 to alpha,1'''
    targets = ((1-alpha)*targets) + alpha*tf.ones(num_classes)

    '''splitting 0 and 1 indexes into separate arrays'''
    a_logits, b_logits = tf.unpack(logits, axis=1)
    a_targets, b_targets = tf.unpack(targets,axis=1)

    targets_magnitude = tf.sqrt(tf.reduce_sum(tf.pow(targets,2),1))
    logits_magnitude = tf.sqrt(tf.reduce_sum(tf.pow(logits,2),1))
    total_magnitude = targets_magnitude*logits_magnitude
    return ((b_targets*b_logits)+(a_targets * a_logits))/total_magnitude

#same as cross entropy for us
'''def log_loss(logits,labels):
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_logits_correct_class = tf.SparseTensor(indices=indices, values=logits,shape=[batch_size, num_classes])
    logits_correct_class = tf.reduce_sum(tf.sparse_tensor_to_dense(sparse_logits_correct_class),1)
    opp_indices = - indices + [1,1]
    sparse_logits_incorrect_class = tf.SparseTensor(indices=opp_indices, values=logits, shape=[batch_size, num_classes])
    logits_incorrect_class = tf.reduce_sum(tf.sparse_tensor_to_dense(sparse_logits_incorrect_class),1)
    return tf.log(1+tf.reduce_sum(tf.exp(logits_correct_class - logits_incorrect_class),axis=1))
'''

def with_memory(parameters_array, func, alpha=0.125, prev_cost):
	'''prev_cost should be initialized to tf.Variable(0)'''
    unprocessed_cost = func(parameters_array)
    return (alpha*unprocessed_cost)+((1-alpha)*prev_cost)


def multiclass_rmse(logits, labels):
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_targets = tf.SparseTensor(indices=indices, values=tf.ones(batch_size, dtype=tf.float32),shape=[batch_size, num_classes])
    targets = tf.sparse_tensor_to_dense(sparse_targets)
    return tf.sqrt(tf.reduce_mean(tf.square(tf.reduce_sum(logits - targets, 1))))
    #tf.contrib.losses.mean_squared_error(y_conv, y_)

def perceptron_loss(logits, labels):
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_targets = tf.SparseTensor(indices=indices, values=tf.ones(batch_size, dtype=tf.float32),shape=[batch_size, num_classes])
    targets = tf.sparse_tensor_to_dense(sparse_targets)
    return 0.5*tf.pow(tf.reduce_sum(logits-targets,1),2)

#unfinished
def difference_cost(parameters_array, parameters_array2, func1, func2):
	return 0

def multiclass_hinge_loss(logits, labels):
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    labels = tf.cast(labels, dtype=tf.int32)
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_targets = tf.SparseTensor(indices=indices, values=tf.ones(batch_size, dtype=tf.float32),shape=[batch_size, num_classes])
    targets = tf.sparse_tensor_to_dense(sparse_targets)

    '''turning targets from 0,1 to -1,1'''
    targets = (2*targets) - tf.ones(num_classes)

    '''splitting 0 and 1 indexes into separate arrays'''
    a_logits, b_logits = tf.unpack(logits, axis=1)
    a_targets, b_targets = tf.unpack(targets,axis=1)
    
    return 0.5*tf.maximum(tf.zeros(batch_size), (b_targets*b_logits)-(a_targets * a_logits))
    #tf.contrib.losses.smoothed_hinge_loss(y_conv, y_)

def sigmoid_cross_entropy(logits, labels):
    labels = tf.cast(labels, dtype=tf.int32)
    batch_size = int(logits.get_shape()[0])
    num_classes = int(logits.get_shape()[1])
    indices = tf.cast(tf.pack((tf.range(0, batch_size), labels), axis=1), dtype=tf.int64)
    sparse_targets = tf.SparseTensor(indices=indices, values=tf.ones(batch_size, dtype=tf.float32),shape=[batch_size, num_classes])
    targets = tf.sparse_tensor_to_dense(sparse_targets)
    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, targets))

